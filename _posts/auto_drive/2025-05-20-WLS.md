---
title: "Weighted Least Squares(가중 최소 제곱법)"
comments: true
categories:
 - auto_drive
header:
 overlay_color: "#e9dcbe"
 overlay_filter: "0.5"
 overlay_image: "/assets/images/bannerimage/car.jpg"
 teaser: "/assets/images/bannerimage/car.jpg"
tags:
 - math
 - linear_algebra
 - auto_drive
 - estimate
 - WLS
sidebar:
 nav: "docs"
toc: true
---

하나가 아닌 여러가지의 계측기를 통해서 나온 관측데이터를 처리하기 위한 방식으로 쓰이는 가중 최소 제곱법에 대해서 알아봅시다.

## 개요

가중 최고 제곱법(Weighted Least Squarws WLS)은 일반적인 최소 제곱법의 확장된 형태로서, 각 관측치에 서로 다른 가중치를 부여하여 회귀 분석을 진행하는 통계적 방법입니다. 이 방법은 특히 데이터의 **이분산성**(오차 항의 분산이 일정하지 않는 경우) 문제가 있을 때 효과적인 해결책을 제공합니다.

## 이론적 배경 및 개념

### 정의

가중 최소 제곱법은 일반 선형 회귀 분석의 확장으로, 관측치마다 다른 가중치를 적용하여 회귀 모형을 추정하는 방법입니다. 기존의 일반 최소 제곱법(Ordinary Least Squares, OLS)이 모든 관측치에 동일한 중요도를 부여하는 반면, 가중 최소 제곱법은 관측치의 신뢰도나 중요도에 따라 다른 가중치를 부여합니다.

### WLS 유도

OLS는 모든 측정의 오차의 분산이 동일하다는 것을 전제로 합니다. 따라서 OLS에서 모든 측정의 오차의 분산이 동일하지 않을때를 가정하고 수정하면 WLS를 구할 수 있습니다.
OLS에서 사용되는 일반적인 선형 모델은 아래와 같습니다.

$$\mathbf{y} = \mathbf{H} \mathbf{x} + \mathbf{v}$$

- $\mathbf{y}$: 측정값 벡터 (m x 1)
- $\mathbf{H}$: 모델 행렬 (m x n)
- $\mathbf{x}$: 추정하려는 파라미터 (n x 1)
- $\mathbf{v}$: 잡음 벡터

OLS에서는 모든 잡음 $\mathbf{v_i}$ 가 동일한 분산을 가진다고 가정합니다. 

$$
E[v_i^2] = \sigma^2 \Rightarrow R = E[v v^T] = \sigma^2 I
$$

그러나 잡음이 독립이지만 분산이 다른 경우를 처리하기 위해서는 잡음 백터 $\mathbf{v}$의 공분산 행렬을 적용해야 합니다.

$$
\mathbb{E}[v_i^2] = \sigma_i^2 \quad \Rightarrow \quad 
\mathbf{R} = 
\begin{bmatrix}
\sigma_1^2 & 0 & \cdots \\
0 & \sigma_2^2 & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix}
$$

잡음 백터의 성분이 **서로 독립**이므로 각각 분산 $\sigma_i^2$​ 을 가진다고 하면, $\mathbf{R}$ 은 **대각선 성분만 있는 행렬 (diagonal matrix)** 이 됩니다.

> 잡음이 클수록 해당 오차 항은 신뢰도가 낮아지므로, 영향력을 작게 해야 하기 때문에 공분산의 역행렬로 오차를 가중합해야 합니다.

$$\mathcal{L}_{WLS}(\mathbf{x}) = \mathbf{e}^T \mathbf{R}^{-1} \mathbf{e}
\quad \text{where } \mathbf{e} = \mathbf{y} - \mathbf{H} \mathbf{x}$$

해당 목적 함수를 전개하면, 오차 벡터로 부터 구한 값이라고 할 수 있습니다.

$$\mathcal{L}_{WLS}(\mathbf{x}) = \sum_{i=1}^{m} \frac{e_i^2}{\sigma_i^2}
\quad \text{where } \mathbf{e} = \mathbf{y} - \mathbf{H} \mathbf{x}
$$

>[!NOTE] WLS 와 OLS
> 
> 만약, 모든 측정 오차의 분산이 동일하다고 가정하면 분산의 역수가 상수값으로 취급이 가능하게 됩니다. 
> $$\mathcal{L}_{WLS}(\mathbf{x}) = \sum_{i=1}^{m} \frac{e_i^2}{\sigma^2} = \frac{1}{\sigma^2} \sum_{i=1}^{m} e_i^2$$  
> 
> 즉 확장해서 작성해보면, 아래와 같게 됩니다.
> 
> $$\mathcal{L}_{WLS}(\mathbf{x}) = \frac{e_1^2}{\sigma^2} + \frac{e_2^2}{\sigma^2} + \cdots + \frac{e_m^2}{\sigma^2} = \frac{1}{\sigma^2}(e_1^2 + e_2^2 + \cdots + e_m^2)$$  
> ​  
> 따라서 모든 측정 오차의 분산이 동일하면, 가중 최소제곱법(WLS)는 일반 최소제곱법(OLS)과 동일한 해를 가지게 됩니다.

OLS와 동일하게 목적함수를 전개하면, 

$$\mathcal{L}_{WLS}(\mathbf{x}) = \mathbf{e}^T \mathbf{R}^{-1} \mathbf{e}
= (\mathbf{y} - \mathbf{H}\mathbf{x})^T \mathbf{R}^{-1} (\mathbf{y} - \mathbf{H}\mathbf{x})
$$

목적 함수가 최소화되는 조건은 목적 함수의 미분값이 0이 되는 지점의 x이므로 x에 관한 값으로 미분하게 되면,

$$
\left. \frac{\partial \mathcal{L}}{\partial \mathbf{x}} \right|_{\mathbf{x} = \hat{\mathbf{x}}} = 0 
= -\mathbf{y}^T \mathbf{R}^{-1} \mathbf{H} + \hat{\mathbf{x}}^T \mathbf{H}^T \mathbf{R}^{-1} \mathbf{H}$$

>[!NOTE] 미분
> 
> 2차 형식의 미분 공식을 적용하면 쉽게 미분 가능합니다.  
> $$\frac{d}{dx} \left[ (y - Hx)^T W (y - Hx) \right] = -2H^T W (y - Hx)$$  

$$\mathbf{H}^T \mathbf{R}^{-1} \mathbf{H} \, \hat{\mathbf{x}}_{WLS} = \mathbf{H}^T \mathbf{R}^{-1} \mathbf{y}$$

따라서 최종 해는 아래와 같이 도출됩니다.

$$\hat{\mathbf{x}} = (\mathbf{H}^T \mathbf{R}^{-1} \mathbf{H})^{-1} \mathbf{H}^T \mathbf{R}^{-1} \mathbf{y}$$

## 요약
- **가중 최소 제곱법 (Weighted Least Squares, WLS)** 은 여러 계측기나 센서로부터 얻은 관측값들이 **서로 다른 신뢰도(분산)** 를 가질 때, 각 관측치에 **적절한 가중치**를 부여하여 추정 성능을 향상시키는 회귀 분석 기법입니다.  
- WLS는 **일반 최소 제곱법(OLS)** 을 확장한 형태로, 오차 항의 분산이 서로 다를 수 있다는 가정을 반영합니다.  
- 각 오차의 분산을 고려하여 목적 함수는 다음과 같이 정의됩니다:  
  $$
  \mathcal{L}_{WLS}(\mathbf{x}) = \sum_{i=1}^{m} \frac{e_i^2}{\sigma_i^2}
  $$
- 분산 행렬 $\mathbf{R}$의 역행렬을 통해 **잡음이 큰 데이터에는 낮은 가중치**, 신뢰도 높은 데이터에는 높은 가중치를 부여합니다.  
- 목적 함수의 최소화 조건을 통해 WLS의 정규 방정식을 도출하고, 최종 해는 다음과 같이 계산됩니다:  
  $$
  \hat{\mathbf{x}} = (\mathbf{H}^T \mathbf{R}^{-1} \mathbf{H})^{-1} \mathbf{H}^T \mathbf{R}^{-1} \mathbf{y}
  $$
- 만약 모든 관측값의 오차 분산이 동일하다면, WLS는 OLS와 **동일한 해**를 갖게 됩니다.  

#### References

[Weighted least squares - Wikipedia](https://en.wikipedia.org/wiki/Weighted_least_squares)  
[Least-Squares Regression](https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html)  
[ms.mcmaster.ca/canty/teaching/stat3a03/Lectures7.pdf](https://ms.mcmaster.ca/canty/teaching/stat3a03/Lectures7.pdf)   